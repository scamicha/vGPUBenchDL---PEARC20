@INPROCEEDINGS{hypervisor,
author={R. {Morabito} and J. {Kj√§llman} and M. {Komu}},
booktitle={2015 IEEE International Conference on Cloud Engineering},
title={Hypervisors vs. Lightweight Virtualization: A Performance Comparison},
year={2015},
volume={},
number={},
pages={386-393},
keywords={operating systems (computers);virtual machines;virtualisation;hypervisors;lightweight virtualization technologies;operating systems;benchmarks tools;strengths;weaknesses;anomalies;processing;storage;memory;network;virtual machines;Albeit containers;Containers;Virtual machine monitors;Virtualization;Linux;Benchmark testing;Operating systems;Performance; Benchmarking; Virtualization; Hypervisor; Container},
doi={10.1109/IC2E.2015.74},
ISSN={null},
month={March},}

@article{genomics,
    author        = "Zou, James and Huss, Mikael and Abid, Abubakar and Mohammadi, Pejman and Torkamani, Ali and Telenti, Amalio",
    title         = "A primer on deep learning in genomics",
    journal       = "Nature Genetics",
    volume        = "51",
    number        = "1",
    year          = "2019",
    pages         = "12--18",
    doi           = "10.1038/s41588-018-0295-5",
    url           = "https://doi.org/10.1038/s41588-018-0295-5"
    }
    
@article{climate,
  author    = {Yunjie Liu and
              Evan Racah and
              Prabhat and
              Joaquin Correa and
              Amir Khosrowshahi and
              David Lavers and
              Kenneth Kunkel and
              Michael F. Wehner and
              William D. Collins},
  title     = {Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets},
  journal   = {CoRR},
  volume    = {abs/1605.01156},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.01156},
  archivePrefix = {arXiv},
  eprint    = {1605.01156},
  timestamp = {Mon, 13 Aug 2018 16:48:40 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LiuRPCKLKWC16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{material-science,
        Abstract = {The inner structure of a material is called microstructure. It stores the genesis of a material and determines all its physical and chemical properties. While microstructural characterization is widely spread and well known, the microstructural classification is mostly done manually by human experts, which gives rise to uncertainties due to subjectivity. Since the microstructure could be a combination of different phases or constituents with complex substructures its automatic classification is very challenging and only a few prior studies exist. Prior works focused on designed and engineered features by experts and classified microstructures separately from the feature extraction step. Recently, Deep Learning methods have shown strong performance in vision applications by learning the features from data together with the classification step. In this work, we propose a Deep Learning method for microstructural classification in the examples of certain microstructural constituents of low carbon steel. This novel method employs pixel-wise segmentation via Fully Convolutional Neural Network (FCNN) accompanied by a max-voting scheme. Our system achieves 93.94{\%} classification accuracy, drastically outperforming the state-of-the-art method of 48.89{\%} accuracy. Beyond the strong performance of our method, this line of research offers a more robust and first of all objective way for the difficult task of steel quality appreciation.},
        Author = {Azimi, Seyed Majid and Britz, Dominik and Engstler, Michael and Fritz, Mario and M{\"u}cklich, Frank},
        Da = {2018/02/01},
        Date-Added = {2019-09-09 11:19:11 -0400},
        Date-Modified = {2019-09-09 11:19:48 -0400},
        Doi = {10.1038/s41598-018-20037-5},
        Id = {Azimi2018},
        Isbn = {2045-2322},
        Journal = {Scientific Reports},
        Number = {1},
        Pages = {2128},
        Title = {Advanced Steel Microstructural Classification by Deep Learning Methods},
        Ty = {JOUR},
        Url = {https://doi.org/10.1038/s41598-018-20037-5},
        Volume = {8},
        Year = {2018},
        Bdsk-Url-1 = {https://doi.org/10.1038/s41598-018-20037-5}}

@inproceedings{chemistry,
 author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
 title = {Neural Message Passing for Quantum Chemistry},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
 series = {ICML'17},
 year = {2017},
 location = {Sydney, NSW, Australia},
 pages = {1263--1272},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3305381.3305512},
 acmid = {3305512},
 publisher = {JMLR.org},
} 

@article{prec-health,
    author = {Beam, Andrew L. and Kohane, Isaac S.},
    title = "{Big Data and Machine Learning in Health Care}",
    journal = {JAMA},
    volume = {319},
    number = {13},
    pages = {1317-1318},
    year = {2018},
    month = {04},
    %abstract = "{Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to\
 %watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non-mac\
 %hine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many s\
%ectors.It is no surprise then that medicine is awash with claims of revolution from the application of machine learning to big health care data. Recent exa\
%mples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians. Though machine learning and big\
 %data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our h\
%ope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and bi\
%g data in health care.}",
    issn = {0098-7484},
    doi = {10.1001/jama.2017.18391},
    url = {https://doi.org/10.1001/jama.2017.18391},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2675024/jama\_beam\_2018\_vp\_170174.pdf},
}

@article{astro,
  title = {Deep neural networks to enable real-time multimessenger astrophysics},
  author = {George, Daniel and Huerta, E. A.},
  journal = {Phys. Rev. D},
  volume = {97},
  issue = {4},
  pages = {044039},
  numpages = {23},
  year = {2018},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.97.044039},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.97.044039}
}

@article{mg-sn-1,
 author = {Xu, Rengan and Tian, Xiaonan and Chandrasekaran, Sunita and Chapman, Barbara},
 title = {Multi-{GPU} Support on Single Node Using Directive-based Programming Model},
 journal = {Sci. Program.},
 issue_date = {January 2015},
 volume = {2015},
 month = jan,
 year = {2016},
 issn = {1058-9244},
 pages = {3:3--3:3},
 articleno = {3},
 numpages = {1},
 url = {https://doi.org/10.1155/2015/621730},
 doi = {10.1155/2015/621730},
 acmid = {2930576},
 publisher = {Hindawi Publishing Corp.},
 address = {New York, NY, United States},
} 

@article{mg-sn-2,
  author    = {Yuechao Pan and
              Yangzihao Wang and
              Yuduo Wu and
              Carl Yang and
              John D. Owens},
  title     = {Multi-{GPU} Graph Analytics},
  journal   = {CoRR},
  volume    = {abs/1504.04804},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.04804},
  archivePrefix = {arXiv},
  eprint    = {1504.04804},
  timestamp = {Mon, 13 Aug 2018 16:46:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PanWWYO15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mg-mn-1,
  abstract = {The recent popularity of deep neural networks (DNNs) has generated a lot of
research interest in performing DNN-related computation efficiently. However,
the primary focus is usually very narrow and limited to (i) inference -- i.e.
how to efficiently execute already trained models and (ii) image classification
networks as the primary benchmark for evaluation.
  Our primary goal in this work is to break this myopic view by (i) proposing a
new benchmark for DNN training, called TBD (TBD is short for Training Benchmark
for DNNs), that uses a representative set of DNN models that cover a wide range
of machine learning applications: image classification, machine translation,
speech recognition, object detection, adversarial networks, reinforcement
learning, and (ii) by performing an extensive performance analysis of training
these different applications on three major deep learning frameworks
(TensorFlow, MXNet, CNTK) across different hardware configurations (single-{GPU},
multi-{GPU}, and multi-machine). TBD currently covers six major application
domains and eight different state-of-the-art models.
  We present a new toolchain for performance analysis for these models that
combines the targeted usage of existing performance analysis tools, careful
selection of new and existing metrics and methodologies to analyze the results,
and utilization of domain specific characteristics of DNN training. We also
build a new set of tools for memory profiling in all three major frameworks;
much needed tools that can finally shed some light on precisely how much memory
is consumed by different data structures (weights, activations, gradients,
workspace) in DNN training. By using our tools and methodologies, we make
several important observations and recommendations on where the future research
and optimization of DNN training should be focused.},
  added-at = {2018-03-20T14:04:26.000+0100},
  author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  biburl = {https://www.bibsonomy.org/bibtex/2e17519fabc6d237885b6480c3d1e56e3/rcb},
  description = {[1803.06905] TBD: Benchmarking and Analyzing Deep Neural Network Training},
  interhash = {cdbecb05662e8fcaea5ac0b5dac56c9a},
  intrahash = {e17519fabc6d237885b6480c3d1e56e3},
  keywords = {analysis benchmark framework nn},
  note = {cite arxiv:1803.06905},
  timestamp = {2018-03-20T14:04:26.000+0100},
  title = {TBD: Benchmarking and Analyzing Deep Neural Network Training},
  url = {http://arxiv.org/abs/1803.06905},
  year = 2018
}

@InProceedings{mg-mn-2,
author="Ma, He
and Mao, Fei
and Taylor, Graham W.",
editor="Desprez, Fr{\'e}d{\'e}ric
and Dutot, Pierre-Fran{\c{c}}ois
and Kaklamanis, Christos
and Marchal, Loris
and Molitorisz, Korbinian
and Ricci, Laura
and Scarano, Vittorio
and Vega-Rodr{\'i}guez, Miguel A.
and Varbanescu, Ana Lucia
and Hunold, Sascha
and Scott, Stephen L.
and Lankes, Stefan
and Weidendorfer, Josef",
title="Theano-MPI: A Theano-Based Distributed Training Framework",
booktitle="Euro-Par 2016: Parallel Processing Workshops",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="800--813",
abstract="We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning (https://github.com/uoguelph-mlrg/Theano-MPI).",
isbn="978-3-319-58943-5"
}


@inproceedings{specaccel,
title = "{SPEC ACCEL}: A standard application suite for measuring hardware accelerator performance",
abstract = "Hybrid nodes with hardware accelerators are becoming very common in systems today. Users often find it difficult to characterize and understand the performance advantage of such accelerators for their applications. The SPEC High Performance Group (HPG) has developed a set of performance metrics to evaluate the performance and power consumption of accelerators for various science applications. The new benchmark comprises two suites of applications written in OpenCL and OpenACC and measures the performance of accelerators with respect to a reference platform. The first set of published results demonstrate the viability and relevance of the new metrics in comparing accelerator performance. This paper discusses the benchmark suites and selected published results in great detail.",
keywords = "Energy measurements, OpenACC, OpenCL, SPEC, SPEC ACCEL",
author = "Guido Juckeland and William Brantley and Sunita Chandrasekaran and Barbara Chapman and Shuai Che and Mathew Colgrove and Huiyu Feng and Alexander Grund and Robert Henschel and Hwu, {Wen-Mei W} and Huian Li and M{\"u}ller, {Matthias S.} and Nagel, {Wolfgang E.} and Maxim Perminov and Pavel Shelepugin and Kevin Skadron and John Stratton and Alexey Titov and Ke Wang and {Van Waveren}, Matthijs and Brian Whitney and Sandra Wienke and Rengan Xu and Kalyan Kumaran",
year = "2015",
month = "1",
day = "1",
doi = "10.1007/978-3-319-17248-4_3",
language = "English (US)",
isbn = "9783319172477",
series = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
publisher = "Springer-Verlag",
pages = "46--67",
editor = "Hammond, {Simon D.} and Jarvis, {Stephen A.} and Wright, {Steven A.}",
booktitle = "High Performance Computing Systems",
}

@Article{vgg,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@article{classify-data,
 author = "Olga Russakovsky${}^1$ and
 Jia Deng${}^1$ and
 Hao Su and
 Jonathan Krause and 
 Sanjeev Satheesh and
 Sean Ma and 
 Zhiheng Huang and
 Andrej Karpathy and
 Aditya Khosla and 
 Michael Bernstein and
 Alexander C. Berg and
 ${\rm{Li~Fei-Fei,~~}}^1{\rm{(equal~contribution)}}$ ",
 title = {ImageNet Large Scale Visual Recognition Challenge},
 year = {2014},
 url = {https://arxiv.org/abs/1409.0575},
 publisher = {ImageNet.org},
}

@article{classify-vgg,
 author = {Karen Simonyan, Andrew Zisserman},
 title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
 volume = {abs/1409.1556},
 journal = {CoRR},
 year = {2014},
 url = {https://arxiv.org/abs/1409.1556},
}

@manual{skylake,
    organization  = "Intel",
    title         = "6th Generation Intel Core Processor Family: Datasheet-Volume 1",
    number        = "332687",
    year          =  2015,
    month         =  8,
    url={https://www.intel.com/content/dam/www/public/us/en/documents \\
    /datasheets/desktop-6th-gen-core-family-datasheet-vol-1.pdf}
}

@manual{v100-specs,
    organization  = "NVIDIA",
    title         = "NVIDIA TESLA V100 {GPU} ARCHITECTURE",
    number        = "WP-08608-001_v1.1",
    year          =  2017,
    month         =  8,
    url          =  {https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf},
}
@manual{vdws,
    organization = "NVIDIA",
    title       = "NVIDIA QUADRO Virtual Data
Center Workstation",
    number      = "185532",
    year        = 2019,
    month       = 4,
    url         = {https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/documents/185532_NVIDIA_Quadro%20vDWS_SolutionOverview_NV_US_WEB.pdf},
}
@conference{bench-design,
 author = {Cedric Bourrasset
 and France Boillod-Cerneux
 and Ludovic Sauge
 and Myrtille Deldossi
 and Fran√ßois Weillenreiter
 and Rajesh Bordawekar
 and Susan Malaika
 and Jean-Armand Broyelle
 and Marc West
 and Brian Belgodere},
 title = "Requirements for an Enterprise {AI} Benchmark",
booktitle ={Tenth TPC Technology Conference on Performance Evaluation \& Benchmarking (TPCTC 2018) },
 year = {2018},
 url = {http://cognitive-science.info/wp-content/uploads/2018/08/Requirements \\ \_for\_a\_Enterprise\_AI\_Benchmark.TPCTC\_.20180831c.pdf},
}

@online{HPL,
author   = "Petitet, A. and Whaley, R. C. and Dongarra, J. and Cleary, A.",
title     = "{HPL} - A Portable Implementation of the High-Performance {L}inpack Benchmark for Distributed-Memory Computers",
url       = "http://www.netlib.org/benchmark/hpl/",
urldate   = {9/11/2019}
}

@online{specweb,
author   = "{Standard Performance Evaluation Corporation}",
title     = "{SPEC ACCEL}",
url       = "https://www.spec.org/accel/",
urldate   = {9/11/2019}
}

@online{vfio,
author  = "{Linux Foundation}",
title   = "{VFIO - "Virtual Function I/O"}",
url     = "https://www.kernel.org/doc/Documentation/vfio.txt",
urldate = "{9/11/2019}"
}

@online{mlperfweb,
author   = "{Standard Performance Evaluation Corporation}",
title     = "{SPEC ACCEL}",
url       = "https://www.spec.org/accel/",
urldate   = {9/11/2019}
}

@inproceedings{jetstream1,
 author = {Stewart, Craig A. and Cockerill, Timothy M. and Foster, Ian and Hancock, David and Merchant, Nirav and Skidmore, Edwin and Stanzione, Daniel and Taylor, James and Tuecke, Steven and Turner, George and Vaughn, Matthew and Gaffney, Niall I.},
 title = {Jetstream: A Self-provisioned, Scalable Science and Engineering Cloud Environment},
 booktitle = {Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
 series = {XSEDE '15},
 year = {2015},
 isbn = {978-1-4503-3720-5},
 location = {St. Louis, Missouri},
 pages = {29:1--29:8},
 articleno = {29},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2792745.2792774},
 doi = {10.1145/2792745.2792774},
 acmid = {2792774},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {atmosphere, big data, cloud computing, long tail of science},
} 

@ARTICLE{jetstream2, 
author={J. {Towns} and T. {Cockerill} and M. {Dahan} and I. {Foster} and K. {Gaither} and A. {Grimshaw} and V. {Hazlewood} and S. {Lathrop} and D. {Lifka} and G. D. {Peterson} and R. {Roskies} and J. R. {Scott} and N. {Wilkins-Diehr}}, 
journal={Computing in Science Engineering}, 
title={XSEDE: Accelerating Scientific Discovery}, 
year={2014}, 
volume={16}, 
number={5}, 
pages={62-74}, 
keywords={engineering computing;natural sciences computing;research and development management;research initiatives;XSEDE;Extreme Science and Engineering Discovery Environment;advanced digital services;digital services;national e-science infrastructure ecosystem;digitally enabled scholars;multidisciplinary collaborations;national e-science infrastructure ecosystem;campus-based resources;scientific discovery;Knowledge discovery;Scientific computing;Digital systems;Materials engineering;Supercomputers;scientific computing;distributed virtual organizations;distributed computing;research infrastructures;HPC;cyberinfrastructure}, 
doi={10.1109/MCSE.2014.80}, 
ISSN={}, 
month={Sep.},}